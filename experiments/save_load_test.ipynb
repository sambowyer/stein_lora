{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the loading and saving of MultiLoRA models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user/work/dg22309/stein_lora/stein_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from stein_lora import MultiLoraConfig, MultiLoraModel\n",
    "import peft\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import AutoModelForCausalLM\n",
    "import torch as t\n",
    "\n",
    "device = t.device(\"cuda\") if t.cuda.is_available() else t.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user/work/dg22309/stein_lora/stein_env/lib/python3.12/site-packages/peft/tuners/lora/layer.py:1091: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load a basic gpt2 model\n",
    "base_model1 = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(device)\n",
    "base_model2 = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(device)\n",
    "# Apply regular LoRA\n",
    "lora_config = LoraConfig(r=4)\n",
    "lora_model = get_peft_model(base_model1, lora_config)\n",
    "\n",
    "# Apply Multi-LoRA\n",
    "multi_lora_config = MultiLoraConfig(r=4, K=5)\n",
    "multi_lora_model = get_peft_model(base_model2, multi_lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the lora model\n",
    "lora_model.save_pretrained(\"temp/lora_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the lora model\n",
    "lora_model2 = AutoModelForCausalLM.from_pretrained(\"temp/lora_model\").to(device) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MultiLoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the multi-lora model\n",
    "\n",
    "# hack to let us save the model\n",
    "multi_lora_model.peft_config['default'].peft_type = peft.PeftType.LORA\n",
    "\n",
    "multi_lora_model.save_pretrained(\"temp/multi_lora_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the config and change the peft_type to multi_lora\n",
    "with open(\"temp/multi_lora_model/adapter_config.json\", \"r\") as f:\n",
    "    config = f.read()\n",
    "\n",
    "config = config.replace('\"peft_type\": \"LORA\"', '\"peft_type\": \"MultiLORA\"')\n",
    "# config = config.replace('\"peft_type\": \"MultiLORA\"', '\"peft_type\": \"LORA\"') # revert the change\n",
    "\n",
    "\n",
    "with open(\"temp/multi_lora_model/adapter_config.json\", \"w\") as f:\n",
    "    f.write(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LORA': <class 'peft.tuners.lora.model.LoraModel'>, 'LOHA': <class 'peft.tuners.loha.model.LoHaModel'>, 'LOKR': <class 'peft.tuners.lokr.model.LoKrModel'>, 'ADALORA': <class 'peft.tuners.adalora.model.AdaLoraModel'>, 'BOFT': <class 'peft.tuners.boft.model.BOFTModel'>, 'IA3': <class 'peft.tuners.ia3.model.IA3Model'>, 'OFT': <class 'peft.tuners.oft.model.OFTModel'>, 'POLY': <class 'peft.tuners.poly.model.PolyModel'>, 'LN_TUNING': <class 'peft.tuners.ln_tuning.model.LNTuningModel'>, 'VERA': <class 'peft.tuners.vera.model.VeraModel'>, 'FOURIERFT': <class 'peft.tuners.fourierft.model.FourierFTModel'>, 'XLORA': <class 'peft.tuners.xlora.model.XLoraModel'>, 'HRA': <class 'peft.tuners.hra.model.HRAModel'>, 'MultiLORA': <class 'stein_lora.multi_lora.MultiLoraModel'>}\n",
      "{'ADAPTION_PROMPT': <class 'peft.tuners.adaption_prompt.config.AdaptionPromptConfig'>, 'PROMPT_TUNING': <class 'peft.tuners.prompt_tuning.config.PromptTuningConfig'>, 'PREFIX_TUNING': <class 'peft.tuners.prefix_tuning.config.PrefixTuningConfig'>, 'P_TUNING': <class 'peft.tuners.p_tuning.config.PromptEncoderConfig'>, 'LORA': <class 'peft.tuners.lora.config.LoraConfig'>, 'LOHA': <class 'peft.tuners.loha.config.LoHaConfig'>, 'LOKR': <class 'peft.tuners.lokr.config.LoKrConfig'>, 'ADALORA': <class 'peft.tuners.adalora.config.AdaLoraConfig'>, 'BOFT': <class 'peft.tuners.boft.config.BOFTConfig'>, 'IA3': <class 'peft.tuners.ia3.config.IA3Config'>, 'MULTITASK_PROMPT_TUNING': <class 'peft.tuners.multitask_prompt_tuning.config.MultitaskPromptTuningConfig'>, 'OFT': <class 'peft.tuners.oft.config.OFTConfig'>, 'POLY': <class 'peft.tuners.poly.config.PolyConfig'>, 'LN_TUNING': <class 'peft.tuners.ln_tuning.config.LNTuningConfig'>, 'VERA': <class 'peft.tuners.vera.config.VeraConfig'>, 'FOURIERFT': <class 'peft.tuners.fourierft.config.FourierFTConfig'>, 'XLORA': <class 'peft.tuners.xlora.config.XLoraConfig'>, 'HRA': <class 'peft.tuners.hra.config.HRAConfig'>, 'MultiLORA': <class 'stein_lora.multi_lora.MultiLoraConfig'>}\n"
     ]
    }
   ],
   "source": [
    "# add MultiLoraModel to the peft tuner mapping\n",
    "peft.mapping.PEFT_TYPE_TO_TUNER_MAPPING['MultiLORA'] = MultiLoraModel\n",
    "peft.mapping.PEFT_TYPE_TO_CONFIG_MAPPING['MultiLORA'] = MultiLoraConfig\n",
    "\n",
    "print(peft.mapping.PEFT_TYPE_TO_TUNER_MAPPING)\n",
    "print(peft.mapping.PEFT_TYPE_TO_CONFIG_MAPPING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change peft.utils.save_and_load.set_peft_model_state_dict(...) to allow for MultiLoraModel\n",
    "\n",
    "def set_peft_model_state_dict_with_multilora(\n",
    "    model, peft_model_state_dict, adapter_name=\"default\", ignore_mismatched_sizes: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Set the state dict of the Peft model.\n",
    "\n",
    "    Args:\n",
    "        model ([`PeftModel`]):\n",
    "            The Peft model.\n",
    "        peft_model_state_dict (`dict`):\n",
    "            The state dict of the Peft model.\n",
    "        adapter_name (`str`, *optional*, defaults to `\"default\"`):\n",
    "            The name of the adapter whose state dict should be set.\n",
    "        ignore_mismatched_sizes (`bool`, *optional*, defaults to `False`):\n",
    "            Whether to ignore mismatched in the state dict.\n",
    "    \"\"\"\n",
    "    config = model.peft_config[adapter_name]\n",
    "    state_dict = {}\n",
    "    if getattr(model, \"modules_to_save\", None) is not None:\n",
    "        for key, value in peft_model_state_dict.items():\n",
    "            if any(module_name in key for module_name in model.modules_to_save):\n",
    "                for module_name in model.modules_to_save:\n",
    "                    if module_name in key:\n",
    "                        key = key.replace(module_name, f\"{module_name}.modules_to_save.{adapter_name}\")\n",
    "                        break\n",
    "            state_dict[key] = value\n",
    "    else:\n",
    "        state_dict = peft_model_state_dict\n",
    "\n",
    "    if config.peft_type in (\n",
    "        PeftType.LORA,\n",
    "        PeftType.LOHA,\n",
    "        PeftType.LOKR,\n",
    "        PeftType.ADALORA,\n",
    "        PeftType.IA3,\n",
    "        PeftType.OFT,\n",
    "        PeftType.POLY,\n",
    "        PeftType.LN_TUNING,\n",
    "        PeftType.BOFT,\n",
    "        PeftType.VERA,\n",
    "        PeftType.FOURIERFT,\n",
    "        PeftType.HRA,\n",
    "        'MultiLORA'      ##################### NEW LINE\n",
    "    ):\n",
    "        peft_model_state_dict = {}\n",
    "        parameter_prefix = {\n",
    "            PeftType.IA3: \"ia3_\",\n",
    "            PeftType.LORA: \"lora_\",\n",
    "            PeftType.ADALORA: \"lora_\",\n",
    "            PeftType.LOHA: \"hada_\",\n",
    "            PeftType.LOKR: \"lokr_\",\n",
    "            PeftType.OFT: \"oft_\",\n",
    "            PeftType.POLY: \"poly_\",\n",
    "            PeftType.BOFT: \"boft_\",\n",
    "            PeftType.LN_TUNING: \"ln_tuning_\",\n",
    "            PeftType.VERA: \"vera_lambda_\",\n",
    "            PeftType.FOURIERFT: \"fourierft_\",\n",
    "            PeftType.HRA: \"hra_\",\n",
    "            'MultiLORA': \"lora_\"     ##################### NEW LINE\n",
    "        }[config.peft_type]\n",
    "        for k, v in state_dict.items():\n",
    "            if parameter_prefix in k:\n",
    "                suffix = k.split(parameter_prefix)[1]\n",
    "                if \".\" in suffix:\n",
    "                    suffix_to_replace = \".\".join(suffix.split(\".\")[1:])\n",
    "                    k = k.replace(suffix_to_replace, f\"{adapter_name}.{suffix_to_replace}\")\n",
    "                else:\n",
    "                    k = f\"{k}.{adapter_name}\"\n",
    "                peft_model_state_dict[k] = v\n",
    "            else:\n",
    "                peft_model_state_dict[k] = v\n",
    "\n",
    "        if config.peft_type == PeftType.ADALORA:\n",
    "            rank_pattern = config.rank_pattern\n",
    "            if rank_pattern is not None:\n",
    "                model.resize_modules_by_rank_pattern(rank_pattern, adapter_name)\n",
    "        elif config.peft_type == PeftType.VERA:\n",
    "            if config.save_projection and \"base_model.vera_A\" not in peft_model_state_dict:\n",
    "                raise ValueError(\n",
    "                    \"Specified to load vera_A and vera_B from state dictionary however they were not present!\"\n",
    "                )\n",
    "            elif not config.save_projection and \"base_model.vera_A\" in peft_model_state_dict:\n",
    "                warnings.warn(\n",
    "                    \"Specified to not load vera_A and vera_B from state dictionary however they are present in state\"\n",
    "                    \" dictionary! Consider using them to ensure checkpoint loading is correct on all platforms using\"\n",
    "                    \" `peft_config.save_projection = True`\"\n",
    "                )\n",
    "            elif not config.save_projection:  # and no vera_A in state dictionary\n",
    "                warnings.warn(\n",
    "                    \"Specified to not load vera_A and vera_B from state dictionary. This means we will be relying on\"\n",
    "                    \" PRNG initialisation to restore these projections using `config.projection_prng_key`, which may\"\n",
    "                    \" not be accurate on all system configurations.\"\n",
    "                )\n",
    "        elif config.peft_type == PeftType.LORA:\n",
    "            # Here we take care of a refactor of DoRA which changed lora_magnitude_vector from a ParameterDict to a\n",
    "            # ModuleDict with a DoraLayer instance. The old parameter is now the \"weight\" attribute of that layer.\n",
    "            old_dora_suffix = f\"lora_magnitude_vector.{adapter_name}\"\n",
    "\n",
    "            def renamed_dora_weights(k):\n",
    "                if k.endswith(old_dora_suffix):\n",
    "                    k = k + \".weight\"\n",
    "                return k\n",
    "\n",
    "            peft_model_state_dict = {renamed_dora_weights(k): v for k, v in peft_model_state_dict.items()}\n",
    "\n",
    "    elif config.is_prompt_learning or config.peft_type == PeftType.ADAPTION_PROMPT:\n",
    "        peft_model_state_dict = state_dict\n",
    "    elif config.peft_type == PeftType.XLORA:\n",
    "        peft_model_state_dict = state_dict\n",
    "    else:\n",
    "        print(config.peft_type)\n",
    "        raise NotImplementedError\n",
    "\n",
    "    peft_model_state_dict, mismatched_keys = _find_mismatched_keys(\n",
    "        model, peft_model_state_dict, ignore_mismatched_sizes=ignore_mismatched_sizes\n",
    "    )\n",
    "    load_result = model.load_state_dict(peft_model_state_dict, strict=False)\n",
    "    if config.is_prompt_learning:\n",
    "        model.prompt_encoder[adapter_name].embedding.load_state_dict(\n",
    "            {\"weight\": peft_model_state_dict[\"prompt_embeddings\"]}, strict=True\n",
    "        )\n",
    "\n",
    "    if config.peft_type == PeftType.MULTITASK_PROMPT_TUNING:\n",
    "        model.prompt_encoder[adapter_name].load_state_dict(peft_model_state_dict, strict=False)\n",
    "\n",
    "    if mismatched_keys:\n",
    "        # see https://github.com/huggingface/transformers/blob/09f9f566de83eef1f13ee83b5a1bbeebde5c80c1/src/transformers/modeling_utils.py#L4039\n",
    "        mismatched_warning = \"\\n\".join(\n",
    "            [\n",
    "                f\"- {key}: found shape {shape1} in the checkpoint and {shape2} in the model instantiated\"\n",
    "                for key, shape1, shape2 in mismatched_keys\n",
    "            ]\n",
    "        )\n",
    "        msg = (\n",
    "            f\"Some weights of {model.__class__.__name__} were not initialized from the model checkpoint \"\n",
    "            f\"and are being ignored because you passed `ignore_mismatched_sizes=True`: {mismatched_warning}.\"\n",
    "        )\n",
    "        warnings.warn(msg)\n",
    "    return load_result\n",
    "\n",
    "peft.utils.save_and_load.set_peft_model_state_dict = set_peft_model_state_dict_with_multilora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Unknown PEFT type passed: MultiLORA",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# load the multi-lora model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m multi_lora_model2 \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemp/multi_lora_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/user/work/dg22309/stein_lora/stein_env/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m/user/work/dg22309/stein_lora/stein_env/lib/python3.12/site-packages/transformers/modeling_utils.py:4022\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4019\u001b[0m     model\u001b[38;5;241m.\u001b[39mhf_quantizer \u001b[38;5;241m=\u001b[39m hf_quantizer\n\u001b[1;32m   4021\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _adapter_model_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 4022\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_adapter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4023\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_adapter_model_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4024\u001b[0m \u001b[43m        \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4025\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4026\u001b[0m \u001b[43m        \u001b[49m\u001b[43madapter_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madapter_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4027\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4029\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_loading_info:\n\u001b[1;32m   4030\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m loading_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/user/work/dg22309/stein_lora/stein_env/lib/python3.12/site-packages/transformers/integrations/peft.py:214\u001b[0m, in \u001b[0;36mPeftAdapterMixin.load_adapter\u001b[0;34m(self, peft_model_id, adapter_name, revision, token, device_map, max_memory, offload_folder, offload_index, peft_config, adapter_state_dict, adapter_kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m     processed_adapter_state_dict[new_key] \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m    213\u001b[0m \u001b[38;5;66;03m# Load state dict\u001b[39;00m\n\u001b[0;32m--> 214\u001b[0m incompatible_keys \u001b[38;5;241m=\u001b[39m \u001b[43mset_peft_model_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessed_adapter_state_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m incompatible_keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# check only for unexpected keys\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(incompatible_keys, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munexpected_keys\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(incompatible_keys\u001b[38;5;241m.\u001b[39munexpected_keys) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/user/work/dg22309/stein_lora/stein_env/lib/python3.12/site-packages/peft/utils/save_and_load.py:390\u001b[0m, in \u001b[0;36mset_peft_model_state_dict\u001b[0;34m(model, peft_model_state_dict, adapter_name, ignore_mismatched_sizes)\u001b[0m\n\u001b[1;32m    388\u001b[0m     peft_model_state_dict \u001b[38;5;241m=\u001b[39m state_dict\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 390\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown PEFT type passed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39mpeft_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    392\u001b[0m peft_model_state_dict, mismatched_keys \u001b[38;5;241m=\u001b[39m _find_mismatched_keys(\n\u001b[1;32m    393\u001b[0m     model, peft_model_state_dict, ignore_mismatched_sizes\u001b[38;5;241m=\u001b[39mignore_mismatched_sizes\n\u001b[1;32m    394\u001b[0m )\n\u001b[1;32m    395\u001b[0m load_result \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mload_state_dict(peft_model_state_dict, strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Unknown PEFT type passed: MultiLORA"
     ]
    }
   ],
   "source": [
    "# load the multi-lora model\n",
    "multi_lora_model2 = AutoModelForCausalLM.from_pretrained(\"temp/multi_lora_model\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load multilora model manually\n",
    "base_model3 = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(device)\n",
    "saved_multilora_config = MultiLoraConfig.from_pretrained(\"temp/multi_lora_model\")\n",
    "\n",
    "multi_lora_model3 = MultiLoraModel(base_model3, saved_multilora_config, adapter_name='default').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiLoraConfig(peft_type='MultiLORA', auto_mapping={'base_model_class': 'GPT2LMHeadModel', 'parent_library': 'transformers.models.gpt2.modeling_gpt2'}, base_model_name_or_path='gpt2', revision=None, task_type=None, inference_mode=True, r=4, target_modules={'c_attn'}, lora_alpha=8, lora_dropout=0.0, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), K=5)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_multilora_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# check if the parameters are the same\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p1, p2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(multi_lora_model\u001b[38;5;241m.\u001b[39mparameters(), multi_lora_model3\u001b[38;5;241m.\u001b[39mparameters()):\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m t\u001b[38;5;241m.\u001b[39mallclose(p1, p2)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# check if the parameters are the same\n",
    "for p1, p2 in zip(multi_lora_model.parameters(), multi_lora_model3.parameters()):\n",
    "    assert t.allclose(p1, p2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stein_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
