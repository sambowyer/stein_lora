{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the loading and saving of MultiLoRA models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user/work/dg22309/stein_lora/stein_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from stein_lora import MultiLoraConfig, MultiLoraModel\n",
    "import peft\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import AutoModelForCausalLM\n",
    "import torch as t\n",
    "\n",
    "device = t.device(\"cuda\") if t.cuda.is_available() else t.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user/work/dg22309/stein_lora/stein_env/lib/python3.12/site-packages/peft/tuners/lora/layer.py:1091: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load a basic gpt2 model\n",
    "base_model1 = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(device)\n",
    "base_model2 = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(device)\n",
    "# Apply regular LoRA\n",
    "lora_config = LoraConfig(r=4)\n",
    "lora_model = get_peft_model(base_model1, lora_config)\n",
    "\n",
    "# Apply Multi-LoRA\n",
    "multi_lora_config = MultiLoraConfig(r=4, K=5)\n",
    "multi_lora_model = get_peft_model(base_model2, multi_lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the lora model\n",
    "lora_model.save_pretrained(\"temp/lora_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the lora model\n",
    "lora_model2 = AutoModelForCausalLM.from_pretrained(\"temp/lora_model\").to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the parameters are the same\n",
    "for p1, p2 in zip(lora_model.parameters(), lora_model2.parameters()):\n",
    "    assert t.allclose(p1, p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.transformer.wte.weight\n",
      "transformer.wte.weight\n",
      "base_model.model.transformer.wpe.weight\n",
      "transformer.wpe.weight\n",
      "base_model.model.transformer.h.0.ln_1.weight\n",
      "transformer.h.0.ln_1.weight\n",
      "base_model.model.transformer.h.0.ln_1.bias\n",
      "transformer.h.0.ln_1.bias\n",
      "base_model.model.transformer.h.0.attn.c_attn.base_layer.weight\n",
      "transformer.h.0.attn.c_attn.base_layer.weight\n",
      "base_model.model.transformer.h.0.attn.c_attn.base_layer.bias\n",
      "transformer.h.0.attn.c_attn.base_layer.bias\n",
      "base_model.model.transformer.h.0.attn.c_attn.lora_A.default.weight\n",
      "transformer.h.0.attn.c_attn.lora_A.default.weight\n",
      "base_model.model.transformer.h.0.attn.c_attn.lora_B.default.weight\n",
      "transformer.h.0.attn.c_attn.lora_B.default.weight\n",
      "base_model.model.transformer.h.0.attn.c_proj.weight\n",
      "transformer.h.0.attn.c_proj.weight\n",
      "base_model.model.transformer.h.0.attn.c_proj.bias\n",
      "transformer.h.0.attn.c_proj.bias\n",
      "base_model.model.transformer.h.0.ln_2.weight\n",
      "transformer.h.0.ln_2.weight\n",
      "base_model.model.transformer.h.0.ln_2.bias\n",
      "transformer.h.0.ln_2.bias\n",
      "base_model.model.transformer.h.0.mlp.c_fc.weight\n",
      "transformer.h.0.mlp.c_fc.weight\n",
      "base_model.model.transformer.h.0.mlp.c_fc.bias\n",
      "transformer.h.0.mlp.c_fc.bias\n",
      "base_model.model.transformer.h.0.mlp.c_proj.weight\n",
      "transformer.h.0.mlp.c_proj.weight\n",
      "base_model.model.transformer.h.0.mlp.c_proj.bias\n",
      "transformer.h.0.mlp.c_proj.bias\n",
      "base_model.model.transformer.h.1.ln_1.weight\n",
      "transformer.h.1.ln_1.weight\n",
      "base_model.model.transformer.h.1.ln_1.bias\n",
      "transformer.h.1.ln_1.bias\n",
      "base_model.model.transformer.h.1.attn.c_attn.base_layer.weight\n",
      "transformer.h.1.attn.c_attn.base_layer.weight\n",
      "base_model.model.transformer.h.1.attn.c_attn.base_layer.bias\n",
      "transformer.h.1.attn.c_attn.base_layer.bias\n",
      "base_model.model.transformer.h.1.attn.c_attn.lora_A.default.weight\n",
      "transformer.h.1.attn.c_attn.lora_A.default.weight\n",
      "base_model.model.transformer.h.1.attn.c_attn.lora_B.default.weight\n",
      "transformer.h.1.attn.c_attn.lora_B.default.weight\n",
      "base_model.model.transformer.h.1.attn.c_proj.weight\n",
      "transformer.h.1.attn.c_proj.weight\n",
      "base_model.model.transformer.h.1.attn.c_proj.bias\n",
      "transformer.h.1.attn.c_proj.bias\n",
      "base_model.model.transformer.h.1.ln_2.weight\n",
      "transformer.h.1.ln_2.weight\n",
      "base_model.model.transformer.h.1.ln_2.bias\n",
      "transformer.h.1.ln_2.bias\n",
      "base_model.model.transformer.h.1.mlp.c_fc.weight\n",
      "transformer.h.1.mlp.c_fc.weight\n",
      "base_model.model.transformer.h.1.mlp.c_fc.bias\n",
      "transformer.h.1.mlp.c_fc.bias\n",
      "base_model.model.transformer.h.1.mlp.c_proj.weight\n",
      "transformer.h.1.mlp.c_proj.weight\n",
      "base_model.model.transformer.h.1.mlp.c_proj.bias\n",
      "transformer.h.1.mlp.c_proj.bias\n",
      "base_model.model.transformer.h.2.ln_1.weight\n",
      "transformer.h.2.ln_1.weight\n",
      "base_model.model.transformer.h.2.ln_1.bias\n",
      "transformer.h.2.ln_1.bias\n",
      "base_model.model.transformer.h.2.attn.c_attn.base_layer.weight\n",
      "transformer.h.2.attn.c_attn.base_layer.weight\n",
      "base_model.model.transformer.h.2.attn.c_attn.base_layer.bias\n",
      "transformer.h.2.attn.c_attn.base_layer.bias\n",
      "base_model.model.transformer.h.2.attn.c_attn.lora_A.default.weight\n",
      "transformer.h.2.attn.c_attn.lora_A.default.weight\n",
      "base_model.model.transformer.h.2.attn.c_attn.lora_B.default.weight\n",
      "transformer.h.2.attn.c_attn.lora_B.default.weight\n",
      "base_model.model.transformer.h.2.attn.c_proj.weight\n",
      "transformer.h.2.attn.c_proj.weight\n",
      "base_model.model.transformer.h.2.attn.c_proj.bias\n",
      "transformer.h.2.attn.c_proj.bias\n",
      "base_model.model.transformer.h.2.ln_2.weight\n",
      "transformer.h.2.ln_2.weight\n",
      "base_model.model.transformer.h.2.ln_2.bias\n",
      "transformer.h.2.ln_2.bias\n",
      "base_model.model.transformer.h.2.mlp.c_fc.weight\n",
      "transformer.h.2.mlp.c_fc.weight\n",
      "base_model.model.transformer.h.2.mlp.c_fc.bias\n",
      "transformer.h.2.mlp.c_fc.bias\n",
      "base_model.model.transformer.h.2.mlp.c_proj.weight\n",
      "transformer.h.2.mlp.c_proj.weight\n",
      "base_model.model.transformer.h.2.mlp.c_proj.bias\n",
      "transformer.h.2.mlp.c_proj.bias\n",
      "base_model.model.transformer.h.3.ln_1.weight\n",
      "transformer.h.3.ln_1.weight\n",
      "base_model.model.transformer.h.3.ln_1.bias\n",
      "transformer.h.3.ln_1.bias\n",
      "base_model.model.transformer.h.3.attn.c_attn.base_layer.weight\n",
      "transformer.h.3.attn.c_attn.base_layer.weight\n",
      "base_model.model.transformer.h.3.attn.c_attn.base_layer.bias\n",
      "transformer.h.3.attn.c_attn.base_layer.bias\n",
      "base_model.model.transformer.h.3.attn.c_attn.lora_A.default.weight\n",
      "transformer.h.3.attn.c_attn.lora_A.default.weight\n",
      "base_model.model.transformer.h.3.attn.c_attn.lora_B.default.weight\n",
      "transformer.h.3.attn.c_attn.lora_B.default.weight\n",
      "base_model.model.transformer.h.3.attn.c_proj.weight\n",
      "transformer.h.3.attn.c_proj.weight\n",
      "base_model.model.transformer.h.3.attn.c_proj.bias\n",
      "transformer.h.3.attn.c_proj.bias\n",
      "base_model.model.transformer.h.3.ln_2.weight\n",
      "transformer.h.3.ln_2.weight\n",
      "base_model.model.transformer.h.3.ln_2.bias\n",
      "transformer.h.3.ln_2.bias\n",
      "base_model.model.transformer.h.3.mlp.c_fc.weight\n",
      "transformer.h.3.mlp.c_fc.weight\n",
      "base_model.model.transformer.h.3.mlp.c_fc.bias\n",
      "transformer.h.3.mlp.c_fc.bias\n",
      "base_model.model.transformer.h.3.mlp.c_proj.weight\n",
      "transformer.h.3.mlp.c_proj.weight\n",
      "base_model.model.transformer.h.3.mlp.c_proj.bias\n",
      "transformer.h.3.mlp.c_proj.bias\n",
      "base_model.model.transformer.h.4.ln_1.weight\n",
      "transformer.h.4.ln_1.weight\n",
      "base_model.model.transformer.h.4.ln_1.bias\n",
      "transformer.h.4.ln_1.bias\n",
      "base_model.model.transformer.h.4.attn.c_attn.base_layer.weight\n",
      "transformer.h.4.attn.c_attn.base_layer.weight\n",
      "base_model.model.transformer.h.4.attn.c_attn.base_layer.bias\n",
      "transformer.h.4.attn.c_attn.base_layer.bias\n",
      "base_model.model.transformer.h.4.attn.c_attn.lora_A.default.weight\n",
      "transformer.h.4.attn.c_attn.lora_A.default.weight\n",
      "base_model.model.transformer.h.4.attn.c_attn.lora_B.default.weight\n",
      "transformer.h.4.attn.c_attn.lora_B.default.weight\n",
      "base_model.model.transformer.h.4.attn.c_proj.weight\n",
      "transformer.h.4.attn.c_proj.weight\n",
      "base_model.model.transformer.h.4.attn.c_proj.bias\n",
      "transformer.h.4.attn.c_proj.bias\n",
      "base_model.model.transformer.h.4.ln_2.weight\n",
      "transformer.h.4.ln_2.weight\n",
      "base_model.model.transformer.h.4.ln_2.bias\n",
      "transformer.h.4.ln_2.bias\n",
      "base_model.model.transformer.h.4.mlp.c_fc.weight\n",
      "transformer.h.4.mlp.c_fc.weight\n",
      "base_model.model.transformer.h.4.mlp.c_fc.bias\n",
      "transformer.h.4.mlp.c_fc.bias\n",
      "base_model.model.transformer.h.4.mlp.c_proj.weight\n",
      "transformer.h.4.mlp.c_proj.weight\n",
      "base_model.model.transformer.h.4.mlp.c_proj.bias\n",
      "transformer.h.4.mlp.c_proj.bias\n",
      "base_model.model.transformer.h.5.ln_1.weight\n",
      "transformer.h.5.ln_1.weight\n",
      "base_model.model.transformer.h.5.ln_1.bias\n",
      "transformer.h.5.ln_1.bias\n",
      "base_model.model.transformer.h.5.attn.c_attn.base_layer.weight\n",
      "transformer.h.5.attn.c_attn.base_layer.weight\n",
      "base_model.model.transformer.h.5.attn.c_attn.base_layer.bias\n",
      "transformer.h.5.attn.c_attn.base_layer.bias\n",
      "base_model.model.transformer.h.5.attn.c_attn.lora_A.default.weight\n",
      "transformer.h.5.attn.c_attn.lora_A.default.weight\n",
      "base_model.model.transformer.h.5.attn.c_attn.lora_B.default.weight\n",
      "transformer.h.5.attn.c_attn.lora_B.default.weight\n",
      "base_model.model.transformer.h.5.attn.c_proj.weight\n",
      "transformer.h.5.attn.c_proj.weight\n",
      "base_model.model.transformer.h.5.attn.c_proj.bias\n",
      "transformer.h.5.attn.c_proj.bias\n",
      "base_model.model.transformer.h.5.ln_2.weight\n",
      "transformer.h.5.ln_2.weight\n",
      "base_model.model.transformer.h.5.ln_2.bias\n",
      "transformer.h.5.ln_2.bias\n",
      "base_model.model.transformer.h.5.mlp.c_fc.weight\n",
      "transformer.h.5.mlp.c_fc.weight\n",
      "base_model.model.transformer.h.5.mlp.c_fc.bias\n",
      "transformer.h.5.mlp.c_fc.bias\n",
      "base_model.model.transformer.h.5.mlp.c_proj.weight\n",
      "transformer.h.5.mlp.c_proj.weight\n",
      "base_model.model.transformer.h.5.mlp.c_proj.bias\n",
      "transformer.h.5.mlp.c_proj.bias\n",
      "base_model.model.transformer.h.6.ln_1.weight\n",
      "transformer.h.6.ln_1.weight\n",
      "base_model.model.transformer.h.6.ln_1.bias\n",
      "transformer.h.6.ln_1.bias\n",
      "base_model.model.transformer.h.6.attn.c_attn.base_layer.weight\n",
      "transformer.h.6.attn.c_attn.base_layer.weight\n",
      "base_model.model.transformer.h.6.attn.c_attn.base_layer.bias\n",
      "transformer.h.6.attn.c_attn.base_layer.bias\n",
      "base_model.model.transformer.h.6.attn.c_attn.lora_A.default.weight\n",
      "transformer.h.6.attn.c_attn.lora_A.default.weight\n",
      "base_model.model.transformer.h.6.attn.c_attn.lora_B.default.weight\n",
      "transformer.h.6.attn.c_attn.lora_B.default.weight\n",
      "base_model.model.transformer.h.6.attn.c_proj.weight\n",
      "transformer.h.6.attn.c_proj.weight\n",
      "base_model.model.transformer.h.6.attn.c_proj.bias\n",
      "transformer.h.6.attn.c_proj.bias\n",
      "base_model.model.transformer.h.6.ln_2.weight\n",
      "transformer.h.6.ln_2.weight\n",
      "base_model.model.transformer.h.6.ln_2.bias\n",
      "transformer.h.6.ln_2.bias\n",
      "base_model.model.transformer.h.6.mlp.c_fc.weight\n",
      "transformer.h.6.mlp.c_fc.weight\n",
      "base_model.model.transformer.h.6.mlp.c_fc.bias\n",
      "transformer.h.6.mlp.c_fc.bias\n",
      "base_model.model.transformer.h.6.mlp.c_proj.weight\n",
      "transformer.h.6.mlp.c_proj.weight\n",
      "base_model.model.transformer.h.6.mlp.c_proj.bias\n",
      "transformer.h.6.mlp.c_proj.bias\n",
      "base_model.model.transformer.h.7.ln_1.weight\n",
      "transformer.h.7.ln_1.weight\n",
      "base_model.model.transformer.h.7.ln_1.bias\n",
      "transformer.h.7.ln_1.bias\n",
      "base_model.model.transformer.h.7.attn.c_attn.base_layer.weight\n",
      "transformer.h.7.attn.c_attn.base_layer.weight\n",
      "base_model.model.transformer.h.7.attn.c_attn.base_layer.bias\n",
      "transformer.h.7.attn.c_attn.base_layer.bias\n",
      "base_model.model.transformer.h.7.attn.c_attn.lora_A.default.weight\n",
      "transformer.h.7.attn.c_attn.lora_A.default.weight\n",
      "base_model.model.transformer.h.7.attn.c_attn.lora_B.default.weight\n",
      "transformer.h.7.attn.c_attn.lora_B.default.weight\n",
      "base_model.model.transformer.h.7.attn.c_proj.weight\n",
      "transformer.h.7.attn.c_proj.weight\n",
      "base_model.model.transformer.h.7.attn.c_proj.bias\n",
      "transformer.h.7.attn.c_proj.bias\n",
      "base_model.model.transformer.h.7.ln_2.weight\n",
      "transformer.h.7.ln_2.weight\n",
      "base_model.model.transformer.h.7.ln_2.bias\n",
      "transformer.h.7.ln_2.bias\n",
      "base_model.model.transformer.h.7.mlp.c_fc.weight\n",
      "transformer.h.7.mlp.c_fc.weight\n",
      "base_model.model.transformer.h.7.mlp.c_fc.bias\n",
      "transformer.h.7.mlp.c_fc.bias\n",
      "base_model.model.transformer.h.7.mlp.c_proj.weight\n",
      "transformer.h.7.mlp.c_proj.weight\n",
      "base_model.model.transformer.h.7.mlp.c_proj.bias\n",
      "transformer.h.7.mlp.c_proj.bias\n",
      "base_model.model.transformer.h.8.ln_1.weight\n",
      "transformer.h.8.ln_1.weight\n",
      "base_model.model.transformer.h.8.ln_1.bias\n",
      "transformer.h.8.ln_1.bias\n",
      "base_model.model.transformer.h.8.attn.c_attn.base_layer.weight\n",
      "transformer.h.8.attn.c_attn.base_layer.weight\n",
      "base_model.model.transformer.h.8.attn.c_attn.base_layer.bias\n",
      "transformer.h.8.attn.c_attn.base_layer.bias\n",
      "base_model.model.transformer.h.8.attn.c_attn.lora_A.default.weight\n",
      "transformer.h.8.attn.c_attn.lora_A.default.weight\n",
      "base_model.model.transformer.h.8.attn.c_attn.lora_B.default.weight\n",
      "transformer.h.8.attn.c_attn.lora_B.default.weight\n",
      "base_model.model.transformer.h.8.attn.c_proj.weight\n",
      "transformer.h.8.attn.c_proj.weight\n",
      "base_model.model.transformer.h.8.attn.c_proj.bias\n",
      "transformer.h.8.attn.c_proj.bias\n",
      "base_model.model.transformer.h.8.ln_2.weight\n",
      "transformer.h.8.ln_2.weight\n",
      "base_model.model.transformer.h.8.ln_2.bias\n",
      "transformer.h.8.ln_2.bias\n",
      "base_model.model.transformer.h.8.mlp.c_fc.weight\n",
      "transformer.h.8.mlp.c_fc.weight\n",
      "base_model.model.transformer.h.8.mlp.c_fc.bias\n",
      "transformer.h.8.mlp.c_fc.bias\n",
      "base_model.model.transformer.h.8.mlp.c_proj.weight\n",
      "transformer.h.8.mlp.c_proj.weight\n",
      "base_model.model.transformer.h.8.mlp.c_proj.bias\n",
      "transformer.h.8.mlp.c_proj.bias\n",
      "base_model.model.transformer.h.9.ln_1.weight\n",
      "transformer.h.9.ln_1.weight\n",
      "base_model.model.transformer.h.9.ln_1.bias\n",
      "transformer.h.9.ln_1.bias\n",
      "base_model.model.transformer.h.9.attn.c_attn.base_layer.weight\n",
      "transformer.h.9.attn.c_attn.base_layer.weight\n",
      "base_model.model.transformer.h.9.attn.c_attn.base_layer.bias\n",
      "transformer.h.9.attn.c_attn.base_layer.bias\n",
      "base_model.model.transformer.h.9.attn.c_attn.lora_A.default.weight\n",
      "transformer.h.9.attn.c_attn.lora_A.default.weight\n",
      "base_model.model.transformer.h.9.attn.c_attn.lora_B.default.weight\n",
      "transformer.h.9.attn.c_attn.lora_B.default.weight\n",
      "base_model.model.transformer.h.9.attn.c_proj.weight\n",
      "transformer.h.9.attn.c_proj.weight\n",
      "base_model.model.transformer.h.9.attn.c_proj.bias\n",
      "transformer.h.9.attn.c_proj.bias\n",
      "base_model.model.transformer.h.9.ln_2.weight\n",
      "transformer.h.9.ln_2.weight\n",
      "base_model.model.transformer.h.9.ln_2.bias\n",
      "transformer.h.9.ln_2.bias\n",
      "base_model.model.transformer.h.9.mlp.c_fc.weight\n",
      "transformer.h.9.mlp.c_fc.weight\n",
      "base_model.model.transformer.h.9.mlp.c_fc.bias\n",
      "transformer.h.9.mlp.c_fc.bias\n",
      "base_model.model.transformer.h.9.mlp.c_proj.weight\n",
      "transformer.h.9.mlp.c_proj.weight\n",
      "base_model.model.transformer.h.9.mlp.c_proj.bias\n",
      "transformer.h.9.mlp.c_proj.bias\n",
      "base_model.model.transformer.h.10.ln_1.weight\n",
      "transformer.h.10.ln_1.weight\n",
      "base_model.model.transformer.h.10.ln_1.bias\n",
      "transformer.h.10.ln_1.bias\n",
      "base_model.model.transformer.h.10.attn.c_attn.base_layer.weight\n",
      "transformer.h.10.attn.c_attn.base_layer.weight\n",
      "base_model.model.transformer.h.10.attn.c_attn.base_layer.bias\n",
      "transformer.h.10.attn.c_attn.base_layer.bias\n",
      "base_model.model.transformer.h.10.attn.c_attn.lora_A.default.weight\n",
      "transformer.h.10.attn.c_attn.lora_A.default.weight\n",
      "base_model.model.transformer.h.10.attn.c_attn.lora_B.default.weight\n",
      "transformer.h.10.attn.c_attn.lora_B.default.weight\n",
      "base_model.model.transformer.h.10.attn.c_proj.weight\n",
      "transformer.h.10.attn.c_proj.weight\n",
      "base_model.model.transformer.h.10.attn.c_proj.bias\n",
      "transformer.h.10.attn.c_proj.bias\n",
      "base_model.model.transformer.h.10.ln_2.weight\n",
      "transformer.h.10.ln_2.weight\n",
      "base_model.model.transformer.h.10.ln_2.bias\n",
      "transformer.h.10.ln_2.bias\n",
      "base_model.model.transformer.h.10.mlp.c_fc.weight\n",
      "transformer.h.10.mlp.c_fc.weight\n",
      "base_model.model.transformer.h.10.mlp.c_fc.bias\n",
      "transformer.h.10.mlp.c_fc.bias\n",
      "base_model.model.transformer.h.10.mlp.c_proj.weight\n",
      "transformer.h.10.mlp.c_proj.weight\n",
      "base_model.model.transformer.h.10.mlp.c_proj.bias\n",
      "transformer.h.10.mlp.c_proj.bias\n",
      "base_model.model.transformer.h.11.ln_1.weight\n",
      "transformer.h.11.ln_1.weight\n",
      "base_model.model.transformer.h.11.ln_1.bias\n",
      "transformer.h.11.ln_1.bias\n",
      "base_model.model.transformer.h.11.attn.c_attn.base_layer.weight\n",
      "transformer.h.11.attn.c_attn.base_layer.weight\n",
      "base_model.model.transformer.h.11.attn.c_attn.base_layer.bias\n",
      "transformer.h.11.attn.c_attn.base_layer.bias\n",
      "base_model.model.transformer.h.11.attn.c_attn.lora_A.default.weight\n",
      "transformer.h.11.attn.c_attn.lora_A.default.weight\n",
      "base_model.model.transformer.h.11.attn.c_attn.lora_B.default.weight\n",
      "transformer.h.11.attn.c_attn.lora_B.default.weight\n",
      "base_model.model.transformer.h.11.attn.c_proj.weight\n",
      "transformer.h.11.attn.c_proj.weight\n",
      "base_model.model.transformer.h.11.attn.c_proj.bias\n",
      "transformer.h.11.attn.c_proj.bias\n",
      "base_model.model.transformer.h.11.ln_2.weight\n",
      "transformer.h.11.ln_2.weight\n",
      "base_model.model.transformer.h.11.ln_2.bias\n",
      "transformer.h.11.ln_2.bias\n",
      "base_model.model.transformer.h.11.mlp.c_fc.weight\n",
      "transformer.h.11.mlp.c_fc.weight\n",
      "base_model.model.transformer.h.11.mlp.c_fc.bias\n",
      "transformer.h.11.mlp.c_fc.bias\n",
      "base_model.model.transformer.h.11.mlp.c_proj.weight\n",
      "transformer.h.11.mlp.c_proj.weight\n",
      "base_model.model.transformer.h.11.mlp.c_proj.bias\n",
      "transformer.h.11.mlp.c_proj.bias\n",
      "base_model.model.transformer.ln_f.weight\n",
      "transformer.ln_f.weight\n",
      "base_model.model.transformer.ln_f.bias\n",
      "transformer.ln_f.bias\n",
      "base_model.model.lm_head.weight\n",
      "lm_head.weight\n"
     ]
    }
   ],
   "source": [
    "for x,y in zip([x for x in lora_model.state_dict()], [x for x in lora_model2.state_dict()]):\n",
    "    print(x)\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m lora_model\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m==\u001b[39m lora_model2\u001b[38;5;241m.\u001b[39mconfig\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m lora_model\u001b[38;5;241m.\u001b[39mpeft_config \u001b[38;5;241m==\u001b[39m lora_model2\u001b[38;5;241m.\u001b[39mpeft_config\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert lora_model.config == lora_model2.config\n",
    "assert lora_model.peft_config == lora_model2.peft_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "assert all(t.allclose(x,y) for x,y in zip([lora_model.state_dict()[x] for x in lora_model.state_dict()],\n",
    "                                 [lora_model2.state_dict()[x] for x in lora_model2.state_dict()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MultiLoRA (hacking around for a solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the multi-lora model\n",
    "\n",
    "# hack to let us save the model\n",
    "multi_lora_model.peft_config['default'].peft_type = peft.PeftType.LORA\n",
    "\n",
    "multi_lora_model.save_pretrained(\"temp/multi_lora_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the config and change the peft_type to multi_lora\n",
    "with open(\"temp/multi_lora_model/adapter_config.json\", \"r\") as f:\n",
    "    config = f.read()\n",
    "\n",
    "config = config.replace('\"peft_type\": \"LORA\"', '\"peft_type\": \"MultiLORA\"')\n",
    "# config = config.replace('\"peft_type\": \"MultiLORA\"', '\"peft_type\": \"LORA\"') # revert the change\n",
    "\n",
    "\n",
    "with open(\"temp/multi_lora_model/adapter_config.json\", \"w\") as f:\n",
    "    f.write(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add MultiLoraModel to the peft tuner mapping\n",
    "peft.mapping.PEFT_TYPE_TO_TUNER_MAPPING['MultiLORA'] = MultiLoraModel\n",
    "peft.mapping.PEFT_TYPE_TO_CONFIG_MAPPING['MultiLORA'] = MultiLoraConfig\n",
    "\n",
    "print(peft.mapping.PEFT_TYPE_TO_TUNER_MAPPING)\n",
    "print(peft.mapping.PEFT_TYPE_TO_CONFIG_MAPPING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change peft.utils.save_and_load.set_peft_model_state_dict(...) to allow for MultiLoraModel\n",
    "\n",
    "def set_peft_model_state_dict_with_multilora(\n",
    "    model, peft_model_state_dict, adapter_name=\"default\", ignore_mismatched_sizes: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Set the state dict of the Peft model.\n",
    "\n",
    "    Args:\n",
    "        model ([`PeftModel`]):\n",
    "            The Peft model.\n",
    "        peft_model_state_dict (`dict`):\n",
    "            The state dict of the Peft model.\n",
    "        adapter_name (`str`, *optional*, defaults to `\"default\"`):\n",
    "            The name of the adapter whose state dict should be set.\n",
    "        ignore_mismatched_sizes (`bool`, *optional*, defaults to `False`):\n",
    "            Whether to ignore mismatched in the state dict.\n",
    "    \"\"\"\n",
    "    config = model.peft_config[adapter_name]\n",
    "    state_dict = {}\n",
    "    if getattr(model, \"modules_to_save\", None) is not None:\n",
    "        for key, value in peft_model_state_dict.items():\n",
    "            if any(module_name in key for module_name in model.modules_to_save):\n",
    "                for module_name in model.modules_to_save:\n",
    "                    if module_name in key:\n",
    "                        key = key.replace(module_name, f\"{module_name}.modules_to_save.{adapter_name}\")\n",
    "                        break\n",
    "            state_dict[key] = value\n",
    "    else:\n",
    "        state_dict = peft_model_state_dict\n",
    "\n",
    "    if config.peft_type in (\n",
    "        PeftType.LORA,\n",
    "        PeftType.LOHA,\n",
    "        PeftType.LOKR,\n",
    "        PeftType.ADALORA,\n",
    "        PeftType.IA3,\n",
    "        PeftType.OFT,\n",
    "        PeftType.POLY,\n",
    "        PeftType.LN_TUNING,\n",
    "        PeftType.BOFT,\n",
    "        PeftType.VERA,\n",
    "        PeftType.FOURIERFT,\n",
    "        PeftType.HRA,\n",
    "        'MultiLORA'      ##################### NEW LINE\n",
    "    ):\n",
    "        peft_model_state_dict = {}\n",
    "        parameter_prefix = {\n",
    "            PeftType.IA3: \"ia3_\",\n",
    "            PeftType.LORA: \"lora_\",\n",
    "            PeftType.ADALORA: \"lora_\",\n",
    "            PeftType.LOHA: \"hada_\",\n",
    "            PeftType.LOKR: \"lokr_\",\n",
    "            PeftType.OFT: \"oft_\",\n",
    "            PeftType.POLY: \"poly_\",\n",
    "            PeftType.BOFT: \"boft_\",\n",
    "            PeftType.LN_TUNING: \"ln_tuning_\",\n",
    "            PeftType.VERA: \"vera_lambda_\",\n",
    "            PeftType.FOURIERFT: \"fourierft_\",\n",
    "            PeftType.HRA: \"hra_\",\n",
    "            'MultiLORA': \"lora_\"     ##################### NEW LINE\n",
    "        }[config.peft_type]\n",
    "        for k, v in state_dict.items():\n",
    "            if parameter_prefix in k:\n",
    "                suffix = k.split(parameter_prefix)[1]\n",
    "                if \".\" in suffix:\n",
    "                    suffix_to_replace = \".\".join(suffix.split(\".\")[1:])\n",
    "                    k = k.replace(suffix_to_replace, f\"{adapter_name}.{suffix_to_replace}\")\n",
    "                else:\n",
    "                    k = f\"{k}.{adapter_name}\"\n",
    "                peft_model_state_dict[k] = v\n",
    "            else:\n",
    "                peft_model_state_dict[k] = v\n",
    "\n",
    "        if config.peft_type == PeftType.ADALORA:\n",
    "            rank_pattern = config.rank_pattern\n",
    "            if rank_pattern is not None:\n",
    "                model.resize_modules_by_rank_pattern(rank_pattern, adapter_name)\n",
    "        elif config.peft_type == PeftType.VERA:\n",
    "            if config.save_projection and \"base_model.vera_A\" not in peft_model_state_dict:\n",
    "                raise ValueError(\n",
    "                    \"Specified to load vera_A and vera_B from state dictionary however they were not present!\"\n",
    "                )\n",
    "            elif not config.save_projection and \"base_model.vera_A\" in peft_model_state_dict:\n",
    "                warnings.warn(\n",
    "                    \"Specified to not load vera_A and vera_B from state dictionary however they are present in state\"\n",
    "                    \" dictionary! Consider using them to ensure checkpoint loading is correct on all platforms using\"\n",
    "                    \" `peft_config.save_projection = True`\"\n",
    "                )\n",
    "            elif not config.save_projection:  # and no vera_A in state dictionary\n",
    "                warnings.warn(\n",
    "                    \"Specified to not load vera_A and vera_B from state dictionary. This means we will be relying on\"\n",
    "                    \" PRNG initialisation to restore these projections using `config.projection_prng_key`, which may\"\n",
    "                    \" not be accurate on all system configurations.\"\n",
    "                )\n",
    "        elif config.peft_type == PeftType.LORA:\n",
    "            # Here we take care of a refactor of DoRA which changed lora_magnitude_vector from a ParameterDict to a\n",
    "            # ModuleDict with a DoraLayer instance. The old parameter is now the \"weight\" attribute of that layer.\n",
    "            old_dora_suffix = f\"lora_magnitude_vector.{adapter_name}\"\n",
    "\n",
    "            def renamed_dora_weights(k):\n",
    "                if k.endswith(old_dora_suffix):\n",
    "                    k = k + \".weight\"\n",
    "                return k\n",
    "\n",
    "            peft_model_state_dict = {renamed_dora_weights(k): v for k, v in peft_model_state_dict.items()}\n",
    "\n",
    "    elif config.is_prompt_learning or config.peft_type == PeftType.ADAPTION_PROMPT:\n",
    "        peft_model_state_dict = state_dict\n",
    "    elif config.peft_type == PeftType.XLORA:\n",
    "        peft_model_state_dict = state_dict\n",
    "    else:\n",
    "        print(config.peft_type)\n",
    "        raise NotImplementedError\n",
    "\n",
    "    peft_model_state_dict, mismatched_keys = _find_mismatched_keys(\n",
    "        model, peft_model_state_dict, ignore_mismatched_sizes=ignore_mismatched_sizes\n",
    "    )\n",
    "    load_result = model.load_state_dict(peft_model_state_dict, strict=False)\n",
    "    if config.is_prompt_learning:\n",
    "        model.prompt_encoder[adapter_name].embedding.load_state_dict(\n",
    "            {\"weight\": peft_model_state_dict[\"prompt_embeddings\"]}, strict=True\n",
    "        )\n",
    "\n",
    "    if config.peft_type == PeftType.MULTITASK_PROMPT_TUNING:\n",
    "        model.prompt_encoder[adapter_name].load_state_dict(peft_model_state_dict, strict=False)\n",
    "\n",
    "    if mismatched_keys:\n",
    "        # see https://github.com/huggingface/transformers/blob/09f9f566de83eef1f13ee83b5a1bbeebde5c80c1/src/transformers/modeling_utils.py#L4039\n",
    "        mismatched_warning = \"\\n\".join(\n",
    "            [\n",
    "                f\"- {key}: found shape {shape1} in the checkpoint and {shape2} in the model instantiated\"\n",
    "                for key, shape1, shape2 in mismatched_keys\n",
    "            ]\n",
    "        )\n",
    "        msg = (\n",
    "            f\"Some weights of {model.__class__.__name__} were not initialized from the model checkpoint \"\n",
    "            f\"and are being ignored because you passed `ignore_mismatched_sizes=True`: {mismatched_warning}.\"\n",
    "        )\n",
    "        warnings.warn(msg)\n",
    "    return load_result\n",
    "\n",
    "peft.utils.save_and_load.set_peft_model_state_dict = set_peft_model_state_dict_with_multilora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the multi-lora model\n",
    "multi_lora_model2 = AutoModelForCausalLM.from_pretrained(\"temp/multi_lora_model\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load multilora model manually\n",
    "base_model3 = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(device)\n",
    "saved_multilora_config = MultiLoraConfig.from_pretrained(\"temp/multi_lora_model\")\n",
    "\n",
    "multi_lora_model3 = MultiLoraModel(base_model3, saved_multilora_config, adapter_name='default').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_multilora_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsitute in the saved lora weights from adapter_model.safetensors\n",
    "state_dict = peft.utils.save_and_load.load_peft_weights(\"temp/multi_lora_model\", adapter_name='default')\n",
    "state_dict = {k: v for k, v in state_dict.items() if \"lora\" in k}\n",
    "# multi_lora_model3.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "# print(state_dict)\n",
    "\n",
    "for k, v in state_dict.items():\n",
    "    lora_key = k.replace(\"base_model.\", \"\").replace(\".weight\", \".default.weight\")\n",
    "    if lora_key in multi_lora_model3.state_dict():\n",
    "        print(k)\n",
    "        multi_lora_model3.state_dict()[lora_key].copy_(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ml3 = [t for t,v  in multi_lora_model3.state_dict.items()]\n",
    "# ml = [t for t in multi_lora_model.parameters()]\n",
    "# state_dict.keys()\n",
    "\n",
    "# ml3[0]\n",
    "\n",
    "ml3 = [p for p in multi_lora_model3.state_dict()]\n",
    "print(multi_lora_model3.state_dict().keys())\n",
    "\n",
    "state_dict['base_model.model.transformer.h.0.attn.c_attn.lora_A.weight']\n",
    "state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the parameters are the same\n",
    "for p1, p2 in zip(multi_lora_model.parameters(), multi_lora_model3.parameters()):\n",
    "    assert t.allclose(p1, p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isinstance(multi_lora_model, MultiLoraModel)\n",
    "multi_lora_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiLORA Correct Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stein_lora import save_multilora_weights, apply_saved_multilora_weights\n",
    "# from stein_lora import *\n",
    "# import stein_lora\n",
    "\n",
    "save_multilora_weights(multi_lora_model, \"temp/multi_lora_model\")\n",
    "# multi_lora_model.base_model.save_pretrained(\"temp/multi_lora_model\")\n",
    "\n",
    "base_model4 = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(device)\n",
    "multi_lora_model4 = apply_saved_multilora_weights(base_model4, \"temp/multi_lora_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the parameters are the same\n",
    "for p1, p2 in zip(multi_lora_model.parameters(), multi_lora_model4.parameters()):\n",
    "    assert t.allclose(p1, p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.transformer.wte.weight\n",
      "model.transformer.wte.weight\n",
      "base_model.model.transformer.wpe.weight\n",
      "model.transformer.wpe.weight\n",
      "base_model.model.transformer.h.0.ln_1.weight\n",
      "model.transformer.h.0.ln_1.weight\n",
      "base_model.model.transformer.h.0.ln_1.bias\n",
      "model.transformer.h.0.ln_1.bias\n",
      "base_model.model.transformer.h.0.attn.c_attn.base_layer.weight\n",
      "model.transformer.h.0.attn.c_attn.base_layer.weight\n",
      "base_model.model.transformer.h.0.attn.c_attn.base_layer.bias\n",
      "model.transformer.h.0.attn.c_attn.base_layer.bias\n",
      "base_model.model.transformer.h.0.attn.c_attn.lora_A.default.weight\n",
      "model.transformer.h.0.attn.c_attn.lora_A.default.weight\n",
      "base_model.model.transformer.h.0.attn.c_attn.lora_B.default.weight\n",
      "model.transformer.h.0.attn.c_attn.lora_B.default.weight\n",
      "base_model.model.transformer.h.0.attn.c_proj.weight\n",
      "model.transformer.h.0.attn.c_proj.weight\n",
      "base_model.model.transformer.h.0.attn.c_proj.bias\n",
      "model.transformer.h.0.attn.c_proj.bias\n",
      "base_model.model.transformer.h.0.ln_2.weight\n",
      "model.transformer.h.0.ln_2.weight\n",
      "base_model.model.transformer.h.0.ln_2.bias\n",
      "model.transformer.h.0.ln_2.bias\n",
      "base_model.model.transformer.h.0.mlp.c_fc.weight\n",
      "model.transformer.h.0.mlp.c_fc.weight\n",
      "base_model.model.transformer.h.0.mlp.c_fc.bias\n",
      "model.transformer.h.0.mlp.c_fc.bias\n",
      "base_model.model.transformer.h.0.mlp.c_proj.weight\n",
      "model.transformer.h.0.mlp.c_proj.weight\n",
      "base_model.model.transformer.h.0.mlp.c_proj.bias\n",
      "model.transformer.h.0.mlp.c_proj.bias\n",
      "base_model.model.transformer.h.1.ln_1.weight\n",
      "model.transformer.h.1.ln_1.weight\n",
      "base_model.model.transformer.h.1.ln_1.bias\n",
      "model.transformer.h.1.ln_1.bias\n",
      "base_model.model.transformer.h.1.attn.c_attn.base_layer.weight\n",
      "model.transformer.h.1.attn.c_attn.base_layer.weight\n",
      "base_model.model.transformer.h.1.attn.c_attn.base_layer.bias\n",
      "model.transformer.h.1.attn.c_attn.base_layer.bias\n",
      "base_model.model.transformer.h.1.attn.c_attn.lora_A.default.weight\n",
      "model.transformer.h.1.attn.c_attn.lora_A.default.weight\n",
      "base_model.model.transformer.h.1.attn.c_attn.lora_B.default.weight\n",
      "model.transformer.h.1.attn.c_attn.lora_B.default.weight\n",
      "base_model.model.transformer.h.1.attn.c_proj.weight\n",
      "model.transformer.h.1.attn.c_proj.weight\n",
      "base_model.model.transformer.h.1.attn.c_proj.bias\n",
      "model.transformer.h.1.attn.c_proj.bias\n",
      "base_model.model.transformer.h.1.ln_2.weight\n",
      "model.transformer.h.1.ln_2.weight\n",
      "base_model.model.transformer.h.1.ln_2.bias\n",
      "model.transformer.h.1.ln_2.bias\n",
      "base_model.model.transformer.h.1.mlp.c_fc.weight\n",
      "model.transformer.h.1.mlp.c_fc.weight\n",
      "base_model.model.transformer.h.1.mlp.c_fc.bias\n",
      "model.transformer.h.1.mlp.c_fc.bias\n",
      "base_model.model.transformer.h.1.mlp.c_proj.weight\n",
      "model.transformer.h.1.mlp.c_proj.weight\n",
      "base_model.model.transformer.h.1.mlp.c_proj.bias\n",
      "model.transformer.h.1.mlp.c_proj.bias\n",
      "base_model.model.transformer.h.2.ln_1.weight\n",
      "model.transformer.h.2.ln_1.weight\n",
      "base_model.model.transformer.h.2.ln_1.bias\n",
      "model.transformer.h.2.ln_1.bias\n",
      "base_model.model.transformer.h.2.attn.c_attn.base_layer.weight\n",
      "model.transformer.h.2.attn.c_attn.base_layer.weight\n",
      "base_model.model.transformer.h.2.attn.c_attn.base_layer.bias\n",
      "model.transformer.h.2.attn.c_attn.base_layer.bias\n",
      "base_model.model.transformer.h.2.attn.c_attn.lora_A.default.weight\n",
      "model.transformer.h.2.attn.c_attn.lora_A.default.weight\n",
      "base_model.model.transformer.h.2.attn.c_attn.lora_B.default.weight\n",
      "model.transformer.h.2.attn.c_attn.lora_B.default.weight\n",
      "base_model.model.transformer.h.2.attn.c_proj.weight\n",
      "model.transformer.h.2.attn.c_proj.weight\n",
      "base_model.model.transformer.h.2.attn.c_proj.bias\n",
      "model.transformer.h.2.attn.c_proj.bias\n",
      "base_model.model.transformer.h.2.ln_2.weight\n",
      "model.transformer.h.2.ln_2.weight\n",
      "base_model.model.transformer.h.2.ln_2.bias\n",
      "model.transformer.h.2.ln_2.bias\n",
      "base_model.model.transformer.h.2.mlp.c_fc.weight\n",
      "model.transformer.h.2.mlp.c_fc.weight\n",
      "base_model.model.transformer.h.2.mlp.c_fc.bias\n",
      "model.transformer.h.2.mlp.c_fc.bias\n",
      "base_model.model.transformer.h.2.mlp.c_proj.weight\n",
      "model.transformer.h.2.mlp.c_proj.weight\n",
      "base_model.model.transformer.h.2.mlp.c_proj.bias\n",
      "model.transformer.h.2.mlp.c_proj.bias\n",
      "base_model.model.transformer.h.3.ln_1.weight\n",
      "model.transformer.h.3.ln_1.weight\n",
      "base_model.model.transformer.h.3.ln_1.bias\n",
      "model.transformer.h.3.ln_1.bias\n",
      "base_model.model.transformer.h.3.attn.c_attn.base_layer.weight\n",
      "model.transformer.h.3.attn.c_attn.base_layer.weight\n",
      "base_model.model.transformer.h.3.attn.c_attn.base_layer.bias\n",
      "model.transformer.h.3.attn.c_attn.base_layer.bias\n",
      "base_model.model.transformer.h.3.attn.c_attn.lora_A.default.weight\n",
      "model.transformer.h.3.attn.c_attn.lora_A.default.weight\n",
      "base_model.model.transformer.h.3.attn.c_attn.lora_B.default.weight\n",
      "model.transformer.h.3.attn.c_attn.lora_B.default.weight\n",
      "base_model.model.transformer.h.3.attn.c_proj.weight\n",
      "model.transformer.h.3.attn.c_proj.weight\n",
      "base_model.model.transformer.h.3.attn.c_proj.bias\n",
      "model.transformer.h.3.attn.c_proj.bias\n",
      "base_model.model.transformer.h.3.ln_2.weight\n",
      "model.transformer.h.3.ln_2.weight\n",
      "base_model.model.transformer.h.3.ln_2.bias\n",
      "model.transformer.h.3.ln_2.bias\n",
      "base_model.model.transformer.h.3.mlp.c_fc.weight\n",
      "model.transformer.h.3.mlp.c_fc.weight\n",
      "base_model.model.transformer.h.3.mlp.c_fc.bias\n",
      "model.transformer.h.3.mlp.c_fc.bias\n",
      "base_model.model.transformer.h.3.mlp.c_proj.weight\n",
      "model.transformer.h.3.mlp.c_proj.weight\n",
      "base_model.model.transformer.h.3.mlp.c_proj.bias\n",
      "model.transformer.h.3.mlp.c_proj.bias\n",
      "base_model.model.transformer.h.4.ln_1.weight\n",
      "model.transformer.h.4.ln_1.weight\n",
      "base_model.model.transformer.h.4.ln_1.bias\n",
      "model.transformer.h.4.ln_1.bias\n",
      "base_model.model.transformer.h.4.attn.c_attn.base_layer.weight\n",
      "model.transformer.h.4.attn.c_attn.base_layer.weight\n",
      "base_model.model.transformer.h.4.attn.c_attn.base_layer.bias\n",
      "model.transformer.h.4.attn.c_attn.base_layer.bias\n",
      "base_model.model.transformer.h.4.attn.c_attn.lora_A.default.weight\n",
      "model.transformer.h.4.attn.c_attn.lora_A.default.weight\n",
      "base_model.model.transformer.h.4.attn.c_attn.lora_B.default.weight\n",
      "model.transformer.h.4.attn.c_attn.lora_B.default.weight\n",
      "base_model.model.transformer.h.4.attn.c_proj.weight\n",
      "model.transformer.h.4.attn.c_proj.weight\n",
      "base_model.model.transformer.h.4.attn.c_proj.bias\n",
      "model.transformer.h.4.attn.c_proj.bias\n",
      "base_model.model.transformer.h.4.ln_2.weight\n",
      "model.transformer.h.4.ln_2.weight\n",
      "base_model.model.transformer.h.4.ln_2.bias\n",
      "model.transformer.h.4.ln_2.bias\n",
      "base_model.model.transformer.h.4.mlp.c_fc.weight\n",
      "model.transformer.h.4.mlp.c_fc.weight\n",
      "base_model.model.transformer.h.4.mlp.c_fc.bias\n",
      "model.transformer.h.4.mlp.c_fc.bias\n",
      "base_model.model.transformer.h.4.mlp.c_proj.weight\n",
      "model.transformer.h.4.mlp.c_proj.weight\n",
      "base_model.model.transformer.h.4.mlp.c_proj.bias\n",
      "model.transformer.h.4.mlp.c_proj.bias\n",
      "base_model.model.transformer.h.5.ln_1.weight\n",
      "model.transformer.h.5.ln_1.weight\n",
      "base_model.model.transformer.h.5.ln_1.bias\n",
      "model.transformer.h.5.ln_1.bias\n",
      "base_model.model.transformer.h.5.attn.c_attn.base_layer.weight\n",
      "model.transformer.h.5.attn.c_attn.base_layer.weight\n",
      "base_model.model.transformer.h.5.attn.c_attn.base_layer.bias\n",
      "model.transformer.h.5.attn.c_attn.base_layer.bias\n",
      "base_model.model.transformer.h.5.attn.c_attn.lora_A.default.weight\n",
      "model.transformer.h.5.attn.c_attn.lora_A.default.weight\n",
      "base_model.model.transformer.h.5.attn.c_attn.lora_B.default.weight\n",
      "model.transformer.h.5.attn.c_attn.lora_B.default.weight\n",
      "base_model.model.transformer.h.5.attn.c_proj.weight\n",
      "model.transformer.h.5.attn.c_proj.weight\n",
      "base_model.model.transformer.h.5.attn.c_proj.bias\n",
      "model.transformer.h.5.attn.c_proj.bias\n",
      "base_model.model.transformer.h.5.ln_2.weight\n",
      "model.transformer.h.5.ln_2.weight\n",
      "base_model.model.transformer.h.5.ln_2.bias\n",
      "model.transformer.h.5.ln_2.bias\n",
      "base_model.model.transformer.h.5.mlp.c_fc.weight\n",
      "model.transformer.h.5.mlp.c_fc.weight\n",
      "base_model.model.transformer.h.5.mlp.c_fc.bias\n",
      "model.transformer.h.5.mlp.c_fc.bias\n",
      "base_model.model.transformer.h.5.mlp.c_proj.weight\n",
      "model.transformer.h.5.mlp.c_proj.weight\n",
      "base_model.model.transformer.h.5.mlp.c_proj.bias\n",
      "model.transformer.h.5.mlp.c_proj.bias\n",
      "base_model.model.transformer.h.6.ln_1.weight\n",
      "model.transformer.h.6.ln_1.weight\n",
      "base_model.model.transformer.h.6.ln_1.bias\n",
      "model.transformer.h.6.ln_1.bias\n",
      "base_model.model.transformer.h.6.attn.c_attn.base_layer.weight\n",
      "model.transformer.h.6.attn.c_attn.base_layer.weight\n",
      "base_model.model.transformer.h.6.attn.c_attn.base_layer.bias\n",
      "model.transformer.h.6.attn.c_attn.base_layer.bias\n",
      "base_model.model.transformer.h.6.attn.c_attn.lora_A.default.weight\n",
      "model.transformer.h.6.attn.c_attn.lora_A.default.weight\n",
      "base_model.model.transformer.h.6.attn.c_attn.lora_B.default.weight\n",
      "model.transformer.h.6.attn.c_attn.lora_B.default.weight\n",
      "base_model.model.transformer.h.6.attn.c_proj.weight\n",
      "model.transformer.h.6.attn.c_proj.weight\n",
      "base_model.model.transformer.h.6.attn.c_proj.bias\n",
      "model.transformer.h.6.attn.c_proj.bias\n",
      "base_model.model.transformer.h.6.ln_2.weight\n",
      "model.transformer.h.6.ln_2.weight\n",
      "base_model.model.transformer.h.6.ln_2.bias\n",
      "model.transformer.h.6.ln_2.bias\n",
      "base_model.model.transformer.h.6.mlp.c_fc.weight\n",
      "model.transformer.h.6.mlp.c_fc.weight\n",
      "base_model.model.transformer.h.6.mlp.c_fc.bias\n",
      "model.transformer.h.6.mlp.c_fc.bias\n",
      "base_model.model.transformer.h.6.mlp.c_proj.weight\n",
      "model.transformer.h.6.mlp.c_proj.weight\n",
      "base_model.model.transformer.h.6.mlp.c_proj.bias\n",
      "model.transformer.h.6.mlp.c_proj.bias\n",
      "base_model.model.transformer.h.7.ln_1.weight\n",
      "model.transformer.h.7.ln_1.weight\n",
      "base_model.model.transformer.h.7.ln_1.bias\n",
      "model.transformer.h.7.ln_1.bias\n",
      "base_model.model.transformer.h.7.attn.c_attn.base_layer.weight\n",
      "model.transformer.h.7.attn.c_attn.base_layer.weight\n",
      "base_model.model.transformer.h.7.attn.c_attn.base_layer.bias\n",
      "model.transformer.h.7.attn.c_attn.base_layer.bias\n",
      "base_model.model.transformer.h.7.attn.c_attn.lora_A.default.weight\n",
      "model.transformer.h.7.attn.c_attn.lora_A.default.weight\n",
      "base_model.model.transformer.h.7.attn.c_attn.lora_B.default.weight\n",
      "model.transformer.h.7.attn.c_attn.lora_B.default.weight\n",
      "base_model.model.transformer.h.7.attn.c_proj.weight\n",
      "model.transformer.h.7.attn.c_proj.weight\n",
      "base_model.model.transformer.h.7.attn.c_proj.bias\n",
      "model.transformer.h.7.attn.c_proj.bias\n",
      "base_model.model.transformer.h.7.ln_2.weight\n",
      "model.transformer.h.7.ln_2.weight\n",
      "base_model.model.transformer.h.7.ln_2.bias\n",
      "model.transformer.h.7.ln_2.bias\n",
      "base_model.model.transformer.h.7.mlp.c_fc.weight\n",
      "model.transformer.h.7.mlp.c_fc.weight\n",
      "base_model.model.transformer.h.7.mlp.c_fc.bias\n",
      "model.transformer.h.7.mlp.c_fc.bias\n",
      "base_model.model.transformer.h.7.mlp.c_proj.weight\n",
      "model.transformer.h.7.mlp.c_proj.weight\n",
      "base_model.model.transformer.h.7.mlp.c_proj.bias\n",
      "model.transformer.h.7.mlp.c_proj.bias\n",
      "base_model.model.transformer.h.8.ln_1.weight\n",
      "model.transformer.h.8.ln_1.weight\n",
      "base_model.model.transformer.h.8.ln_1.bias\n",
      "model.transformer.h.8.ln_1.bias\n",
      "base_model.model.transformer.h.8.attn.c_attn.base_layer.weight\n",
      "model.transformer.h.8.attn.c_attn.base_layer.weight\n",
      "base_model.model.transformer.h.8.attn.c_attn.base_layer.bias\n",
      "model.transformer.h.8.attn.c_attn.base_layer.bias\n",
      "base_model.model.transformer.h.8.attn.c_attn.lora_A.default.weight\n",
      "model.transformer.h.8.attn.c_attn.lora_A.default.weight\n",
      "base_model.model.transformer.h.8.attn.c_attn.lora_B.default.weight\n",
      "model.transformer.h.8.attn.c_attn.lora_B.default.weight\n",
      "base_model.model.transformer.h.8.attn.c_proj.weight\n",
      "model.transformer.h.8.attn.c_proj.weight\n",
      "base_model.model.transformer.h.8.attn.c_proj.bias\n",
      "model.transformer.h.8.attn.c_proj.bias\n",
      "base_model.model.transformer.h.8.ln_2.weight\n",
      "model.transformer.h.8.ln_2.weight\n",
      "base_model.model.transformer.h.8.ln_2.bias\n",
      "model.transformer.h.8.ln_2.bias\n",
      "base_model.model.transformer.h.8.mlp.c_fc.weight\n",
      "model.transformer.h.8.mlp.c_fc.weight\n",
      "base_model.model.transformer.h.8.mlp.c_fc.bias\n",
      "model.transformer.h.8.mlp.c_fc.bias\n",
      "base_model.model.transformer.h.8.mlp.c_proj.weight\n",
      "model.transformer.h.8.mlp.c_proj.weight\n",
      "base_model.model.transformer.h.8.mlp.c_proj.bias\n",
      "model.transformer.h.8.mlp.c_proj.bias\n",
      "base_model.model.transformer.h.9.ln_1.weight\n",
      "model.transformer.h.9.ln_1.weight\n",
      "base_model.model.transformer.h.9.ln_1.bias\n",
      "model.transformer.h.9.ln_1.bias\n",
      "base_model.model.transformer.h.9.attn.c_attn.base_layer.weight\n",
      "model.transformer.h.9.attn.c_attn.base_layer.weight\n",
      "base_model.model.transformer.h.9.attn.c_attn.base_layer.bias\n",
      "model.transformer.h.9.attn.c_attn.base_layer.bias\n",
      "base_model.model.transformer.h.9.attn.c_attn.lora_A.default.weight\n",
      "model.transformer.h.9.attn.c_attn.lora_A.default.weight\n",
      "base_model.model.transformer.h.9.attn.c_attn.lora_B.default.weight\n",
      "model.transformer.h.9.attn.c_attn.lora_B.default.weight\n",
      "base_model.model.transformer.h.9.attn.c_proj.weight\n",
      "model.transformer.h.9.attn.c_proj.weight\n",
      "base_model.model.transformer.h.9.attn.c_proj.bias\n",
      "model.transformer.h.9.attn.c_proj.bias\n",
      "base_model.model.transformer.h.9.ln_2.weight\n",
      "model.transformer.h.9.ln_2.weight\n",
      "base_model.model.transformer.h.9.ln_2.bias\n",
      "model.transformer.h.9.ln_2.bias\n",
      "base_model.model.transformer.h.9.mlp.c_fc.weight\n",
      "model.transformer.h.9.mlp.c_fc.weight\n",
      "base_model.model.transformer.h.9.mlp.c_fc.bias\n",
      "model.transformer.h.9.mlp.c_fc.bias\n",
      "base_model.model.transformer.h.9.mlp.c_proj.weight\n",
      "model.transformer.h.9.mlp.c_proj.weight\n",
      "base_model.model.transformer.h.9.mlp.c_proj.bias\n",
      "model.transformer.h.9.mlp.c_proj.bias\n",
      "base_model.model.transformer.h.10.ln_1.weight\n",
      "model.transformer.h.10.ln_1.weight\n",
      "base_model.model.transformer.h.10.ln_1.bias\n",
      "model.transformer.h.10.ln_1.bias\n",
      "base_model.model.transformer.h.10.attn.c_attn.base_layer.weight\n",
      "model.transformer.h.10.attn.c_attn.base_layer.weight\n",
      "base_model.model.transformer.h.10.attn.c_attn.base_layer.bias\n",
      "model.transformer.h.10.attn.c_attn.base_layer.bias\n",
      "base_model.model.transformer.h.10.attn.c_attn.lora_A.default.weight\n",
      "model.transformer.h.10.attn.c_attn.lora_A.default.weight\n",
      "base_model.model.transformer.h.10.attn.c_attn.lora_B.default.weight\n",
      "model.transformer.h.10.attn.c_attn.lora_B.default.weight\n",
      "base_model.model.transformer.h.10.attn.c_proj.weight\n",
      "model.transformer.h.10.attn.c_proj.weight\n",
      "base_model.model.transformer.h.10.attn.c_proj.bias\n",
      "model.transformer.h.10.attn.c_proj.bias\n",
      "base_model.model.transformer.h.10.ln_2.weight\n",
      "model.transformer.h.10.ln_2.weight\n",
      "base_model.model.transformer.h.10.ln_2.bias\n",
      "model.transformer.h.10.ln_2.bias\n",
      "base_model.model.transformer.h.10.mlp.c_fc.weight\n",
      "model.transformer.h.10.mlp.c_fc.weight\n",
      "base_model.model.transformer.h.10.mlp.c_fc.bias\n",
      "model.transformer.h.10.mlp.c_fc.bias\n",
      "base_model.model.transformer.h.10.mlp.c_proj.weight\n",
      "model.transformer.h.10.mlp.c_proj.weight\n",
      "base_model.model.transformer.h.10.mlp.c_proj.bias\n",
      "model.transformer.h.10.mlp.c_proj.bias\n",
      "base_model.model.transformer.h.11.ln_1.weight\n",
      "model.transformer.h.11.ln_1.weight\n",
      "base_model.model.transformer.h.11.ln_1.bias\n",
      "model.transformer.h.11.ln_1.bias\n",
      "base_model.model.transformer.h.11.attn.c_attn.base_layer.weight\n",
      "model.transformer.h.11.attn.c_attn.base_layer.weight\n",
      "base_model.model.transformer.h.11.attn.c_attn.base_layer.bias\n",
      "model.transformer.h.11.attn.c_attn.base_layer.bias\n",
      "base_model.model.transformer.h.11.attn.c_attn.lora_A.default.weight\n",
      "model.transformer.h.11.attn.c_attn.lora_A.default.weight\n",
      "base_model.model.transformer.h.11.attn.c_attn.lora_B.default.weight\n",
      "model.transformer.h.11.attn.c_attn.lora_B.default.weight\n",
      "base_model.model.transformer.h.11.attn.c_proj.weight\n",
      "model.transformer.h.11.attn.c_proj.weight\n",
      "base_model.model.transformer.h.11.attn.c_proj.bias\n",
      "model.transformer.h.11.attn.c_proj.bias\n",
      "base_model.model.transformer.h.11.ln_2.weight\n",
      "model.transformer.h.11.ln_2.weight\n",
      "base_model.model.transformer.h.11.ln_2.bias\n",
      "model.transformer.h.11.ln_2.bias\n",
      "base_model.model.transformer.h.11.mlp.c_fc.weight\n",
      "model.transformer.h.11.mlp.c_fc.weight\n",
      "base_model.model.transformer.h.11.mlp.c_fc.bias\n",
      "model.transformer.h.11.mlp.c_fc.bias\n",
      "base_model.model.transformer.h.11.mlp.c_proj.weight\n",
      "model.transformer.h.11.mlp.c_proj.weight\n",
      "base_model.model.transformer.h.11.mlp.c_proj.bias\n",
      "model.transformer.h.11.mlp.c_proj.bias\n",
      "base_model.model.transformer.ln_f.weight\n",
      "model.transformer.ln_f.weight\n",
      "base_model.model.transformer.ln_f.bias\n",
      "model.transformer.ln_f.bias\n",
      "base_model.model.lm_head.weight\n",
      "model.lm_head.weight\n"
     ]
    }
   ],
   "source": [
    "for x,y in zip([x for x in multi_lora_model.state_dict()], [x for x in multi_lora_model4.state_dict()]):\n",
    "    print(x)\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m multi_lora_model\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m==\u001b[39m multi_lora_model4\u001b[38;5;241m.\u001b[39mconfig\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m multi_lora_model\u001b[38;5;241m.\u001b[39mpeft_config \u001b[38;5;241m==\u001b[39m multi_lora_model4\u001b[38;5;241m.\u001b[39mpeft_config\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert multi_lora_model.config == multi_lora_model4.config\n",
    "assert multi_lora_model.peft_config == multi_lora_model4.peft_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'default': MultiLoraConfig(peft_type='MultiLORA', auto_mapping=None, base_model_name_or_path='gpt2', revision=None, task_type=None, inference_mode=False, r=4, target_modules={'c_attn'}, lora_alpha=8, lora_dropout=0.0, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), K=5)}\n",
      "{'default': MultiLoraConfig(peft_type='MultiLORA', auto_mapping={'base_model_class': 'GPT2LMHeadModel', 'parent_library': 'transformers.models.gpt2.modeling_gpt2'}, base_model_name_or_path='gpt2', revision=None, task_type=None, inference_mode=True, r=4, target_modules={'c_attn'}, lora_alpha=8, lora_dropout=0.0, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), K=5)}\n"
     ]
    }
   ],
   "source": [
    "print(multi_lora_model.peft_config)\n",
    "print(multi_lora_model4.peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all(t.allclose(x,y) for x,y in zip([multi_lora_model.state_dict()[x]  for x in multi_lora_model.state_dict()],\n",
    "                                          [multi_lora_model4.state_dict()[x] for x in multi_lora_model4.state_dict()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What happens if we re-save a model and then load it again?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the state_dict keys change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stein_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
